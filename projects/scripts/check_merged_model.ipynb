{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/kaggle_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(152064, 5120)\n",
      "  (layers): ModuleList(\n",
      "    (0-47): 48 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2SdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "        (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "        (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "        (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model_path = \"../model_output/embedder_icl_finetune_qwen14b_ep2_ds2_fix_emb_old_saver/merged_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, quantization_config=bnb_config, device_map=device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2Model(\n",
      "      (embed_tokens): Embedding(152064, 5120)\n",
      "      (layers): ModuleList(\n",
      "        (0-47): 48 x Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2SdpaAttention(\n",
      "            (q_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (k_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (v_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (o_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (rotary_emb): Qwen2RotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=13824, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (up_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=13824, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (down_proj): lora.Linear4bit(\n",
      "              (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=13824, out_features=32, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "lora_path = \"../model_output/embedder_icl_finetune_qwen14b_ep2_ds2_fix_emb_old_saver/lora_epoch_2\"\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "lora_model = AutoModel.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\", quantization_config=bnb_config, device_map=device)\n",
    "lora_model = PeftModel.from_pretrained(model, lora_path, is_trainable=False)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "print(lora_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPast(last_hidden_state=tensor([[[-1.3877, -0.4512, -1.1846,  ..., -0.1032,  0.5142, -0.4790],\n",
      "         [ 0.5884, -0.0981, -2.5371,  ...,  0.0469, -0.0531, -0.4192],\n",
      "         [ 0.8369,  1.5498, -1.8779,  ...,  0.5288, -0.1210, -0.9419],\n",
      "         ...,\n",
      "         [-0.3696, -2.7891,  1.2588,  ..., -0.2013,  1.8174, -0.6025],\n",
      "         [ 1.9043, -2.4219,  1.6826,  ..., -0.0124,  0.9883,  1.2617],\n",
      "         [ 1.4287, -2.4805,  2.4883,  ...,  0.9897,  1.2217,  1.1953]]],\n",
      "       device='cuda:0', dtype=torch.float16), past_key_values=None, hidden_states=None, attentions=None)\n",
      "BaseModelOutputWithPast(last_hidden_state=tensor([[[-1.3877, -0.4512, -1.1846,  ..., -0.1032,  0.5142, -0.4790],\n",
      "         [ 0.5884, -0.0981, -2.5371,  ...,  0.0469, -0.0531, -0.4192],\n",
      "         [ 0.8369,  1.5498, -1.8779,  ...,  0.5288, -0.1210, -0.9419],\n",
      "         ...,\n",
      "         [-0.3696, -2.7891,  1.2588,  ..., -0.2013,  1.8174, -0.6025],\n",
      "         [ 1.9043, -2.4219,  1.6826,  ..., -0.0124,  0.9883,  1.2617],\n",
      "         [ 1.4287, -2.4805,  2.4883,  ...,  0.9897,  1.2217,  1.1953]]],\n",
      "       device='cuda:0', dtype=torch.float16), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "txt = \"What is the sum of 1 and 2?\"\n",
    "inputs = tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "lora_inputs = lora_tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "lora_outputs = lora_model(**lora_inputs)\n",
    "print(lora_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/kaggle_nlp/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPast(last_hidden_state=tensor([[[-1.3418, -0.4719, -1.3691,  ..., -0.0950,  0.4719, -0.8677],\n",
      "         [-0.2158, -0.5044, -4.1523,  ...,  0.3062, -0.5068, -1.2725],\n",
      "         [ 0.5977,  2.0566, -2.3086,  ...,  1.3379,  0.0278, -0.9976],\n",
      "         ...,\n",
      "         [-2.4219, -0.9570,  0.4705,  ..., -0.5708,  2.7812, -1.5029],\n",
      "         [ 1.4199, -1.9814,  0.2976,  ..., -0.7939,  2.1152,  0.8828],\n",
      "         [ 0.5498, -1.2559,  1.1729,  ...,  0.9023,  1.3574,  1.5029]]],\n",
      "       device='cuda:0', dtype=torch.float16), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "lora_model = lora_model.merge_and_unload()\n",
    "\n",
    "lora_inputs = lora_tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "lora_outputs = lora_model(**lora_inputs)\n",
    "print(lora_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 92, 90, 88]\n"
     ]
    }
   ],
   "source": [
    "# Given scores and their original indexes\n",
    "scores = [90, 85, 88, 92]\n",
    "original_indexes = [2, 0, 3, 1]\n",
    "\n",
    "# Pair each score with its original index\n",
    "paired_list = list(zip(scores, original_indexes))\n",
    "\n",
    "# Sort the paired list based on the original indexes\n",
    "sorted_paired_list = sorted(paired_list, key=lambda x: x[1])\n",
    "\n",
    "# Extract the scores in original order\n",
    "original_order_scores = [score for score, index in sorted_paired_list]\n",
    "\n",
    "# Display the result\n",
    "print(original_order_scores)\n",
    "# Output: [85, 92, 90, 88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/kaggle_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"What is the sum of 1 and 2?\"\n",
    "len(tokenizer(txt)['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 16, 16]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [['What is tasdas1 and 2?', 'What ?'], ['What is the sum of 1 and 2?', 'What d 2?'], ['What is the sum of 1 and 2?', 'What d 2?']]\n",
    "len_pairs = [len(tokenizer(pair[0])['input_ids']) + len(tokenizer(pair[1])['input_ids']) for pair in pairs]\n",
    "len_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices = sorted(range(len(len_pairs)), key=lambda k: len_pairs[k], reverse=True)\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67687/2163294830.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  args = torch.load(args_path)\n",
      "/root/miniconda3/envs/kaggle_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "args_path = '/root/autodl-tmp/github/FlagEmbedding/projects/model_output/reranker_ft_qwen14b_ep4_4gpu/training_args.bin'\n",
    "args = torch.load(args_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: ../model_output/reranker_ft_qwen14b_ep4_4gpu\n",
      "overwrite_output_dir: True\n",
      "do_train: False\n",
      "do_eval: False\n",
      "do_predict: False\n",
      "eval_strategy: no\n",
      "prediction_loss_only: False\n",
      "per_device_train_batch_size: 1\n",
      "per_device_eval_batch_size: 1\n",
      "per_gpu_train_batch_size: None\n",
      "per_gpu_eval_batch_size: None\n",
      "gradient_accumulation_steps: 8\n",
      "eval_accumulation_steps: None\n",
      "eval_delay: 0\n",
      "torch_empty_cache_steps: None\n",
      "learning_rate: 0.0002\n",
      "weight_decay: 0.01\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_epsilon: 1e-08\n",
      "max_grad_norm: 1.0\n",
      "num_train_epochs: 4.0\n",
      "max_steps: -1\n",
      "lr_scheduler_type: linear\n",
      "lr_scheduler_kwargs: {}\n",
      "warmup_ratio: 0.05\n",
      "warmup_steps: 0\n",
      "log_level: passive\n",
      "log_level_replica: warning\n",
      "log_on_each_node: True\n",
      "logging_dir: ../model_output/reranker_ft_qwen14b_ep4_4gpu/runs/Dec10_12-35-29_autodl-container-bf1a4f8116-4d59960b\n",
      "logging_strategy: steps\n",
      "logging_first_step: False\n",
      "logging_steps: 1.0\n",
      "logging_nan_inf_filter: True\n",
      "save_strategy: steps\n",
      "save_steps: 500\n",
      "save_total_limit: 5\n",
      "save_safetensors: True\n",
      "save_on_each_node: False\n",
      "save_only_model: False\n",
      "restore_callback_states_from_checkpoint: False\n",
      "no_cuda: False\n",
      "use_cpu: False\n",
      "use_mps_device: False\n",
      "seed: 42\n",
      "data_seed: None\n",
      "jit_mode_eval: False\n",
      "use_ipex: False\n",
      "bf16: True\n",
      "fp16: False\n",
      "fp16_opt_level: O1\n",
      "half_precision_backend: auto\n",
      "bf16_full_eval: False\n",
      "fp16_full_eval: False\n",
      "tf32: None\n",
      "local_rank: 0\n",
      "ddp_backend: None\n",
      "tpu_num_cores: None\n",
      "tpu_metrics_debug: False\n",
      "debug: []\n",
      "dataloader_drop_last: True\n",
      "eval_steps: None\n",
      "dataloader_num_workers: 0\n",
      "dataloader_prefetch_factor: None\n",
      "past_index: -1\n",
      "run_name: ../model_output/reranker_ft_qwen14b_ep4_4gpu\n",
      "disable_tqdm: False\n",
      "remove_unused_columns: True\n",
      "label_names: None\n",
      "load_best_model_at_end: False\n",
      "metric_for_best_model: None\n",
      "greater_is_better: None\n",
      "ignore_data_skip: False\n",
      "fsdp: []\n",
      "fsdp_min_num_params: 0\n",
      "fsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "fsdp_transformer_layer_cls_to_wrap: None\n",
      "accelerator_config: AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False)\n",
      "deepspeed: ./ds_stage2.json\n",
      "label_smoothing_factor: 0.0\n",
      "optim: adamw_torch\n",
      "optim_args: None\n",
      "adafactor: False\n",
      "group_by_length: False\n",
      "length_column_name: length\n",
      "report_to: ['wandb']\n",
      "ddp_find_unused_parameters: None\n",
      "ddp_bucket_cap_mb: None\n",
      "ddp_broadcast_buffers: None\n",
      "dataloader_pin_memory: True\n",
      "dataloader_persistent_workers: False\n",
      "skip_memory_metrics: True\n",
      "use_legacy_prediction_loop: False\n",
      "push_to_hub: False\n",
      "resume_from_checkpoint: None\n",
      "hub_model_id: None\n",
      "hub_strategy: every_save\n",
      "hub_token: None\n",
      "hub_private_repo: False\n",
      "hub_always_push: False\n",
      "gradient_checkpointing: True\n",
      "gradient_checkpointing_kwargs: None\n",
      "include_inputs_for_metrics: False\n",
      "eval_do_concat_batches: True\n",
      "fp16_backend: auto\n",
      "evaluation_strategy: None\n",
      "push_to_hub_model_id: None\n",
      "push_to_hub_organization: None\n",
      "push_to_hub_token: None\n",
      "mp_parameters: \n",
      "auto_find_batch_size: False\n",
      "full_determinism: False\n",
      "torchdynamo: None\n",
      "ray_scope: last\n",
      "ddp_timeout: 1800\n",
      "torch_compile: False\n",
      "torch_compile_backend: None\n",
      "torch_compile_mode: None\n",
      "dispatch_batches: None\n",
      "split_batches: None\n",
      "include_tokens_per_second: False\n",
      "include_num_input_tokens_seen: False\n",
      "neftune_noise_alpha: None\n",
      "optim_target_modules: None\n",
      "batch_eval_metrics: False\n",
      "eval_on_start: False\n",
      "eval_use_gather_object: False\n",
      "sub_batch_size: None\n",
      "save_lora_every_epoch: True\n",
      "distributed_state: Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "_n_gpu: 1\n",
      "__cached__setup_devices: cuda:0\n",
      "deepspeed_plugin: DeepSpeedPlugin(hf_ds_config=<transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x7fc795827550>, gradient_accumulation_steps=8, gradient_clipping=1.0, zero_stage=2, is_train_batch_min=True, offload_optimizer_device='none', offload_param_device='none', offload_optimizer_nvme_path='none', offload_param_nvme_path='none', zero3_init_flag=False, zero3_save_16bit_model=False, transformer_moe_cls_names=None, enable_msamp=False, msamp_opt_level='O1')\n",
      "hf_deepspeed_config: <transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x7fc795827550>\n"
     ]
    }
   ],
   "source": [
    "args_dict = vars(args)\n",
    "for key, value in args_dict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
